{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvPA9fIty5c",
        "outputId": "9f4a5b1c-c3ba-418a-8dd4-84fa3ad92301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 714, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 714 (delta 7), reused 0 (delta 0), pack-reused 687 (from 3)\u001b[K\n",
            "Receiving objects: 100% (714/714), 999.10 KiB | 2.99 MiB/s, done.\n",
            "Resolving deltas: 100% (392/392), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/alexkalitenko125/nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cs9c2P1fvRn",
        "outputId": "185d8e37-4f00-4cae-ce58-35db2e7002e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/data/feynman/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoMQ8IMt8Bu",
        "outputId": "b28ac704-1c3c-415c-afb7-aebb30b63570"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 13,211 tokens\n",
            "val has 1,504 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/ && python train.py --dtype=float16 --dataset=feynman --compile=False --n_layer=4 --n_head=4 --n_embd=64 --block_size=64 --batch_size=8 --init_from=gpt2 --eval_interval=100 --eval_iters=100 --max_iters=100 --bias=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXoH_7tunr6",
        "outputId": "f1fcc2ba-1aaa-422b-d172-0fd28a30af65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: dataset = feynman\n",
            "Overriding: compile = False\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: init_from = gpt2\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: eval_iters = 100\n",
            "Overriding: max_iters = 100\n",
            "Overriding: bias = True\n",
            "tokens per iteration will be: 20,480\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "2025-01-05 17:07:41.802727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-05 17:07:41.822362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-05 17:07:41.828243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-05 17:07:41.843152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-05 17:07:43.065339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.04MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:07<00:00, 75.9MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 917kB/s]\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 50, with 123,581,184 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.0363, val loss 4.3070\n",
            "iter 0: loss 4.0088, time 6343.86ms, mfu -100.00%\n",
            "iter 1: loss 3.9793, time 1745.42ms, mfu -100.00%\n",
            "iter 2: loss 3.8928, time 1845.00ms, mfu -100.00%\n",
            "iter 3: loss 4.0123, time 1849.76ms, mfu -100.00%\n",
            "iter 4: loss 3.9645, time 1855.89ms, mfu -100.00%\n",
            "iter 5: loss 3.9346, time 1858.00ms, mfu 2.65%\n",
            "iter 6: loss 3.9440, time 1867.19ms, mfu 2.64%\n",
            "iter 7: loss 3.8878, time 1860.87ms, mfu 2.64%\n",
            "iter 8: loss 4.1489, time 1871.98ms, mfu 2.64%\n",
            "iter 9: loss 3.9528, time 1874.01ms, mfu 2.64%\n",
            "iter 10: loss 3.9571, time 1879.99ms, mfu 2.64%\n",
            "iter 11: loss 3.7002, time 1886.84ms, mfu 2.63%\n",
            "iter 12: loss 4.0844, time 1892.64ms, mfu 2.63%\n",
            "iter 13: loss 4.1477, time 1904.14ms, mfu 2.63%\n",
            "iter 14: loss 3.5547, time 1897.61ms, mfu 2.62%\n",
            "iter 15: loss 3.7394, time 1904.84ms, mfu 2.62%\n",
            "iter 16: loss 3.5979, time 1915.57ms, mfu 2.61%\n",
            "iter 17: loss 3.7450, time 1917.52ms, mfu 2.61%\n",
            "iter 18: loss 3.8282, time 1925.28ms, mfu 2.60%\n",
            "iter 19: loss 3.7182, time 1938.32ms, mfu 2.60%\n",
            "iter 20: loss 3.5839, time 1942.77ms, mfu 2.59%\n",
            "iter 21: loss 3.5240, time 1943.09ms, mfu 2.58%\n",
            "iter 22: loss 3.6872, time 1940.78ms, mfu 2.58%\n",
            "iter 23: loss 3.4924, time 1949.96ms, mfu 2.57%\n",
            "iter 24: loss 3.5520, time 1955.27ms, mfu 2.57%\n",
            "iter 25: loss 3.5921, time 1963.25ms, mfu 2.56%\n",
            "iter 26: loss 3.7871, time 1961.19ms, mfu 2.56%\n",
            "iter 27: loss 3.4139, time 1963.36ms, mfu 2.55%\n",
            "iter 28: loss 3.4780, time 1966.34ms, mfu 2.55%\n",
            "iter 29: loss 3.5119, time 1964.50ms, mfu 2.54%\n",
            "iter 30: loss 3.5465, time 1966.59ms, mfu 2.54%\n",
            "iter 31: loss 3.4161, time 1967.78ms, mfu 2.53%\n",
            "iter 32: loss 3.5311, time 1963.81ms, mfu 2.53%\n",
            "iter 33: loss 3.4990, time 1961.96ms, mfu 2.53%\n",
            "iter 34: loss 3.2641, time 1959.48ms, mfu 2.53%\n",
            "iter 35: loss 3.4530, time 1953.88ms, mfu 2.52%\n",
            "iter 36: loss 3.3204, time 1948.99ms, mfu 2.52%\n",
            "iter 37: loss 3.2541, time 1947.31ms, mfu 2.52%\n",
            "iter 38: loss 3.1817, time 1946.90ms, mfu 2.52%\n",
            "iter 39: loss 3.3521, time 1940.57ms, mfu 2.53%\n",
            "iter 40: loss 3.2040, time 1942.34ms, mfu 2.53%\n",
            "iter 41: loss 3.4371, time 1932.72ms, mfu 2.53%\n",
            "iter 42: loss 3.4625, time 1928.01ms, mfu 2.53%\n",
            "iter 43: loss 3.3514, time 1931.38ms, mfu 2.53%\n",
            "iter 44: loss 3.2184, time 1934.12ms, mfu 2.53%\n",
            "iter 45: loss 3.1919, time 1923.92ms, mfu 2.54%\n",
            "iter 46: loss 3.3465, time 1922.00ms, mfu 2.54%\n",
            "iter 47: loss 3.4738, time 1922.47ms, mfu 2.54%\n",
            "iter 48: loss 3.0879, time 1913.17ms, mfu 2.54%\n",
            "iter 49: loss 2.9699, time 1921.09ms, mfu 2.54%\n",
            "iter 50: loss 3.3189, time 1925.88ms, mfu 2.54%\n",
            "iter 51: loss 3.0419, time 1918.71ms, mfu 2.55%\n",
            "iter 52: loss 3.0955, time 1925.27ms, mfu 2.55%\n",
            "iter 53: loss 3.0035, time 1925.02ms, mfu 2.55%\n",
            "iter 54: loss 2.8697, time 1919.94ms, mfu 2.55%\n",
            "iter 55: loss 2.9181, time 1919.09ms, mfu 2.55%\n",
            "iter 56: loss 3.0961, time 1932.62ms, mfu 2.55%\n",
            "iter 57: loss 3.0251, time 1923.12ms, mfu 2.55%\n",
            "iter 58: loss 2.8967, time 1923.29ms, mfu 2.55%\n",
            "iter 59: loss 2.9938, time 1925.72ms, mfu 2.55%\n",
            "iter 60: loss 2.8271, time 1925.15ms, mfu 2.55%\n",
            "iter 61: loss 2.8540, time 1915.38ms, mfu 2.55%\n",
            "iter 62: loss 2.9233, time 1921.56ms, mfu 2.55%\n",
            "iter 63: loss 2.9032, time 1928.06ms, mfu 2.55%\n",
            "iter 64: loss 2.9133, time 1932.96ms, mfu 2.55%\n",
            "iter 65: loss 2.8363, time 1926.07ms, mfu 2.55%\n",
            "iter 66: loss 2.6958, time 1941.74ms, mfu 2.55%\n",
            "iter 67: loss 2.7885, time 1931.22ms, mfu 2.55%\n",
            "iter 68: loss 2.7842, time 1936.35ms, mfu 2.55%\n",
            "iter 69: loss 2.6801, time 1933.94ms, mfu 2.55%\n",
            "iter 70: loss 2.5128, time 1929.78ms, mfu 2.55%\n",
            "iter 71: loss 2.5813, time 1932.10ms, mfu 2.55%\n",
            "iter 72: loss 2.6559, time 1939.97ms, mfu 2.55%\n",
            "iter 73: loss 2.5489, time 1943.72ms, mfu 2.54%\n",
            "iter 74: loss 2.6479, time 1935.18ms, mfu 2.54%\n",
            "iter 75: loss 2.5028, time 1936.84ms, mfu 2.54%\n",
            "iter 76: loss 2.4671, time 1933.41ms, mfu 2.54%\n",
            "iter 77: loss 2.5766, time 1934.17ms, mfu 2.54%\n",
            "iter 78: loss 2.6059, time 1933.30ms, mfu 2.54%\n",
            "iter 79: loss 2.4475, time 1942.37ms, mfu 2.54%\n",
            "iter 80: loss 2.3985, time 1938.60ms, mfu 2.54%\n",
            "iter 81: loss 2.4511, time 1935.39ms, mfu 2.54%\n",
            "iter 82: loss 2.2709, time 1941.00ms, mfu 2.54%\n",
            "iter 83: loss 2.3793, time 1934.30ms, mfu 2.54%\n",
            "iter 84: loss 2.2529, time 1934.29ms, mfu 2.54%\n",
            "iter 85: loss 2.1678, time 1941.72ms, mfu 2.54%\n",
            "iter 86: loss 2.1906, time 1933.72ms, mfu 2.54%\n",
            "iter 87: loss 2.2958, time 1937.89ms, mfu 2.54%\n",
            "iter 88: loss 2.1423, time 1931.24ms, mfu 2.54%\n",
            "iter 89: loss 2.2023, time 1929.47ms, mfu 2.54%\n",
            "iter 90: loss 2.0785, time 1931.12ms, mfu 2.54%\n",
            "iter 91: loss 2.2298, time 1933.13ms, mfu 2.54%\n",
            "iter 92: loss 2.3163, time 1935.64ms, mfu 2.54%\n",
            "iter 93: loss 2.1480, time 1935.48ms, mfu 2.54%\n",
            "iter 94: loss 2.0359, time 1935.82ms, mfu 2.54%\n",
            "iter 95: loss 2.0169, time 1933.44ms, mfu 2.54%\n",
            "iter 96: loss 1.9439, time 1938.59ms, mfu 2.54%\n",
            "iter 97: loss 1.9614, time 1931.09ms, mfu 2.54%\n",
            "iter 98: loss 1.8933, time 1932.99ms, mfu 2.54%\n",
            "iter 99: loss 1.7445, time 1934.88ms, mfu 2.54%\n",
            "step 100: train loss 1.8758, val loss 4.3236\n",
            "saving checkpoint to out\n",
            "iter 100: loss 1.7444, time 18967.37ms, mfu 2.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT && python sample.py --dtype=float16 --num_samples=1 --max_new_tokens=100 --start=\"This course\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOjpslgu6g4Y",
        "outputId": "af595a76-c848-4664-afb6-ef2103c80ee8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: start = This course\n",
            "/content/nanoGPT/sample.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "This course is sometimes called the law of physics, and it is so remarkable that it has so much power.\n",
            "The whole subject of physics is now under way in this great country, and we are\n",
            "not going to get into the details of the great problems which we must deal with in\n",
            "this wonderful country of ours, but we can give a brief description of a particular\n",
            "of those particular problems.\n",
            "Now, a physicist is a physicist with a theory. It is a special kind\n",
            "of knowledge,\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}